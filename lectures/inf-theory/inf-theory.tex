%\documentclass[11pt,a4paper]{article}
%\usepackage{fullpage}
%\usepackage{beamerarticle}
%\documentclass[handout,xcolor=pdftex,dvipsnames,table]{beamer}
\documentclass[hyperref={unicode=true}]{beamer}

%\usepackage{pgfpages} 
%\pgfpagesuselayout{resize}[a4paper,border shrink=5mm,landscape] 

\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{../clrscode3e} 
%\usepackage[all]{xy}
\usepackage{colortbl}
%\usepackage{xcolor}
\usepackage{pstricks, pst-tree, pst-node}
\usepackage{epsfig}
\usepackage{multicol}
\usepackage{array}
\usepackage{wrapfig}
%\usepackage{listings}

\definecolor{orange}{cmyk}{0,0.52,1,0}

%\usepackage{beamerthemesplit}

\AtBeginSection[]
{
  \begin{frame}<beamer>{Раздел}
    \tableofcontents[currentsection]
  \end{frame}
}


\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Раздел}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}


\newtheorem{rtheorem}{Теорема} 
\newtheorem{rdefinition}{Определение} 
\newtheorem{rconsequence}{Следствие} 
%default}
%themesplit}

\title{Введение в теорию информации. Сжатия тестов.}
\subtitle{Дискретный анализ 2012/13}
\author{Андрей Калинин, Татьяна Романова}
\date{13 апреля 2013\,г.}
\usetheme{default}
%\usefonttheme{serif}
\usefonttheme[onlymath]{serif}
%\usefonttheme{professionalfonts}
%\usetheme{default} 

\begin{document}

\frame{\titlepage}

%\section[Содержание]{}
%\frame{\tableofcontents}

%\section{Введение в теорию информации}

\section{Базовые понятия теории вероятностей}
\frame{
  \frametitle{Вероятность}
  Задано пространство элементарных событий (конечное или бесконечное):
  \[
  \Omega = \{ \omega_1, \omega_2, \ldots \}
  \]
  и функция $p(\omega_i)\in [0,1]$, удовлетворяющая условию
  нормировки: 
\[
\sum p(\omega_i) = 1.
\]
\begin{itemize}
  \item Значения $p(\omega_i)$ --- вероятности элементарных событий
    $\omega_i$.
  \item Множества $A \subset \omega$ --- события и их вероятность:
\[
P(A)=\sum_{\omega_i\in A}p(\omega_i).
\]
\end{itemize}
}

\frame{
  \frametitle{На практике}
  \begin{itemize}
    \item Возможность проведения экспериментов, исходом которых
      является наступление $\omega_i$ (событие $A$ наступает, если
      наступает $\omega_i \in A$)
    \item Связь $\Omega$ с субэлементарным уровнем, например, если
      событие $n$-кратное бросание монеты, то можно выделить связь с
      однократным бросанием.
    \item Возможность проведения серии опытов, в результате чего:
\[
\frac{N(A)}{N}\rightarrow P(A) \qquad \text{ при }N\rightarrow \infty.
\]
  \end{itemize}
}

\frame{
  \frametitle{Объединение и пересечение событий}
  \[
  P(A+B)=P(A)+P(B)-P(AB)
  \]
  Если события не пересекаются, то
  \[
  P(A+B)=P(A)+P(B)
  \]
}

\frame{
  \frametitle{Условная вероятность}
  \begin{itemize}
  \item $P(B|A)$ --- вероятность наступления $B$ при условии наступления в
  то же время события $A$, или условная вероятность. 
  \item При этом, из всех
  $\omega_i\in A$ рассматриваются только $\omega_i\in B$ и $P(B|A)$
  равнялось бы $P(AB)$, если бы $P(A)=1$. Нормируя, получаем:
  \[
  P(B|A)=\frac{P(AB)}{P(A)}.
  \]
\item Или, формула умножения вероятностей:
  \[
  P(AB)=P(A)P(B|A).
  \]
  \end{itemize}
}

\frame[plain]{
  \frametitle{Формула Байеса}
  Разбиение $\Omega$ на полную группу несовместимых событий $A_1,
  \ldots, A_n$ позволяет любое событие $B$ записать в виде 
  \[
  B = BA_1 + \cdots + BA_n,
  \]
  откуда $P(B)=P(BA_1)+\cdots + P(BA_n),$ и получается формула
  полной вероятности:
  \[
  P(B)=P(B|A_1)P(A_1)+\cdots + P(B|A_n)P(A_n).
  \]
  Пусть $P(A),P(B)>0$. Тогда из 
  \[
  P(AB)=P(A|B)P(B)=P(B|A)P(A)
  \]
вытекает 
\[
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
\]
что приводит к формуле Байеса:
\[
P(A_j|B)=\frac{P(B|A_j)P(A_j)}{\sum_kP(B|A_k)P(A_k)}.
\]
}


\frame{
  \frametitle{Примеры}
  \begin{itemize}
  \item Имеются три карточки. На одной --- с обоих сторон $A$, на другой
  --- $B$. На третьей картонке с одной стороны $A$, с другой ---
  $B$. Одна из картонок выбирается наугад и кладётся на
  стол. Предположим, на видимой стороне картонки оказывается буква
  $A$. Какова вероятность того, что на другой стороне --- тоже $A$?
\pause
  \item Некоторая болезнь имеется у 1 человека на 100. Имеется тест,
    определяющий наличие болезни с 95\%-ой точностью (т.е., в 5\%
    случаев срабатывания теста его результат неверен). Допустим, для
    тестирования случайно отбирают людей и тест <<сработал>> на
    каком-то человеке. Какова вероятность того, что он болен этой болезнью?
  \end{itemize}
}

\frame{
  \frametitle{Парадокс Монти Холла}
  \begin{itemize}
  \item Представьте, что вы стали участником игры, в которой вам нужно выбрать одну из трех дверей. 
         За одной из дверей находится автомобиль, за двумя другими дверями — козы.
         Вы выбираете одну из дверей, например, номер 1, после этого ведущий, который знает, 
         где находится автомобиль, а где — козы, открывает одну из оставшихся дверей, 
         например, номер 3, за которой находится коза.
         После этого он спрашивает вас, не желаете ли вы изменить свой выбор и выбрать дверь номер 2.
         Увеличатся ли ваши шансы выиграть автомобиль, если вы примете предложение ведущего и измените свой выбор?
  \end{itemize}
}

\frame{
  \frametitle{Независимость}
  События $A$ и $B$ называют независимыми, если $P(B|A)=P(B)$, т.е. 
  \[
  P(AB)=P(A)P(B).
  \]
}

\frame{
  \frametitle{Случайные величины}
  \begin{itemize}
  \item Числовая функция $X(\omega)$, заданная на $\Omega$ представляет
  собой случайную величину (например, 1 при выпадении герба, 0 ---
  решки). 
  \item Среднее значение $m_x=\mathrm{E}(X)$,
    \[
    \mathrm{E}(X)=\sum_{\omega\in\Omega} X(\omega)P(\omega),
    \]
    называют математическим ожиданием $X(\omega).$
  \item Математическое ожидание линейно:
\[
\mathrm{E}(\alpha X + \beta Y) = \alpha \mathrm{E}(X) + \beta \mathrm{E}(Y).
\]
  \end{itemize}
}

\frame[plain]{
  \frametitle{Парадокс ожидания серии}
  \begin{itemize}
  \item Какая комбинация, 00 или 01, появится раньше в случайной
    <<01>>-последовательности?
  \item Очевидно, равновероятно, т.к. после первого нуля появление 0
    или 1 --- вероятность 1/2. 
\pause
  \item Однако, число шагов (среднее время ожидания) $m_{00}$ и
    $m_{01}$ --- не одинаково:
    \begin{itemize}
      \item Пусть $m_0$ --- среднее число шагов до 01 при условии, что
        первая цифра последовательности была 0, а $m_1$ --- то же
        самое, но для 1. Тогда:
\[
m_0 = 1 + \frac{1}{2} + \frac{1}{2}m_0, \qquad m_1 = 1 +
\frac{1}{2}m_0 + \frac{1}{2}m_1,
\]
откуда $m_0=3$, $m_1=5$ и $m_{01}=.5(m_0+m_1) = 4$.
\item Если же $m_0^*$ и $m_1^*$ аналоги $m_0$ и $m_1$ в для $00$, то:
\[
m_0^* = 1 + \frac{1}{2} + \frac{1}{2}m_1^*, \qquad m_1^* = 1 +
\frac{1}{2}m_0^* + \frac{1}{2}m_1^*,
\]
и в конечном итоге это даёт $m_{00}=.5(m_0^*+m_1^*) = 6$.
    \end{itemize}
  \end{itemize}
}

\frame{
  \frametitle{Петербургский парадокс}
  \begin{itemize}
  \item Если герб при неоднократном бросании монеты выпадает первый
    раз в $n$-й попытке, участнику игры выплачивается $2^n$ рублей. 
  \item Математическое ожидание выигрыша,
\[
2\cdot \frac{1}{2} + 4\cdot\frac{1}{4} + \cdots +
2^n\cdot\frac{1}{2^n} + \cdots = 1 + 1 + \cdots,
\]
бесконечно. 
\item Сколько можно заплатить за участие в игре?
  \pause
\item Кажется, что сколько угодно --- казино проиграет. 
\item Но это <<в среднем>>, а единичная игра скорее не принесёт
  большого выигрыша и платить большие деньги не имеет смысла.  

  \end{itemize}
}

%
% Наивный
%   Байесовский
%     Классификатор!
%

\section{Наивный байесовский классификатор}

\frame{
  \frametitle{Задача классификации}
  \begin{itemize}
  \item $C$ --- некоторый класс, к которому можно отнести наблюдаемый
    объект. Любой объект относится либо к $C$, либо к $\lnot C$. 
  \item $F_1, \ldots, F_n$ --- факторы, некоторые наблюдаемые
    характеристики объекта. 
  \item Задача --- при неких полученных значениях факторов определить
    принадлежность события к классу, т.е. вычислить:
    \[
    P(C|F_1, \ldots, F_n).
    \]
  \item  Количесто факторов может быть велико. 
  \item Примеры факторов: слова в тексте, результаты измерений,
    телеметрические данные и т.п. 
  \end{itemize}
}

\frame[plain]{
  \frametitle{Модель}
  \begin{itemize}
  \item Из формулы условной вероятности:
    \[
    P(C|F_1,\ldots, F_n) = \frac{P(C)P(F_1, \ldots, F_n|C)}{P(F_1,
      \ldots, F_n)}
    \]
  \item Для оценки вероятности принадлежности к классу $C$ интересено
    только $P(C)P(F_1, \ldots, F_n|C)$, т.к. $P(F_1, \ldots,
    F_n)$ не зависит от класса. 
  \item При этом $P(C)P(F_1, \ldots, F_n|C) = P(C,F_1, \ldots, F_n)$.
  \item Тогда:
    \[
    \begin{split}
    P(C,F_1, \ldots, F_n) &= P(C)P(F_1, \ldots, F_n|C) = \\
    &= P(C)P(F_1,C)P(F_2,\ldots, F_n, |C,F_1) = \\
    &= P(C)P(F_1,C)P(F_2|C,F_1)\times \\&\qquad\times P(F_3,\ldots, F_n, |C,F_1,F_2) = \\
    &= P(C)P(F_1,C)P(F_2|C,F_1)P(F_3|C,F_1,F_2)\times \cdots\\&\qquad \times P(F_n|C,F_1,F_2,F_3,\ldots,F_{n-1}).
    \end{split}
    \]
  \end{itemize}
}

\frame[plain]{
  \frametitle{Наивная модель}
  \begin{itemize}
  \item Наивное предпололжение: предполагаем независимость
    факторов друг от друга.
    \item Т.к. факторы независимы, то для $i \neq j$:
      \[
      P(F_i|C,F_j) = P(F_i|C)
      \]
    \item Тогда:
      \[
      \begin{split}
      P(C,F_1,\ldots, F_n) &= P(C)P(F_1|C)P(F_2|C)P(F_3|C)\cdots
      P(F_n|C) \\
      &= P(C)\prod_{i=1}^n P(F_i|C).
      \end{split}
      \]
    \item И вся модель:
      \[
      P(C|F_1, \ldots, F_n) = \frac{1}{Z}P(C)\prod_{i=1}^n P(F_i|C)
      \]
      где $Z$ зависит только от $F_1, \ldots, F_n$, т.е. константа при
      заданных значениях факторов. 
%      \item Для $k$ классов построить $k$
%      классификаторов, каждый для своего  класса. 
  \end{itemize}
}

\frame{
  \frametitle{Классификация}
  \begin{itemize}
  \item Предварительно нужно оценить вероятности $P(F_i|C)$. 
  \item После чего:
    \[
    \mathrm{classify}(f_1, \ldots, f_n) = \arg \max_c P(C=c)
    \prod_{i=1}^n P(F_i=f_i| C=c).
    \]
  \end{itemize}
}

\frame{
  \frametitle{Классификация текстовых документов}
  \begin{itemize}
  \item Характеристика --- наличие слова в тексте,  т.е. нужно
    предварительно оценить 
\[
P(w_i|C).
\]
\item Предполагаем, что слова никак не связаны друг с другом, их вес не зависит от
длины документа и т.п. 
Тогда:
\[
P(D|C)=\prod_i P(w_i|C)
\]
\item Для условной вероятности:
\[
P(C|D)=\frac{P(C)}{P(D)} P(D|C).
\]
  \end{itemize}
}

\frame{
  \frametitle{Два класса}
  \begin{itemize}
  \item Допустим, есть только два класса, $S$ и $\lnot S$.
  \item Тогда можно оценить принадлежность документа к каждому из
    классов:
    \[
    \begin{split}
      P(D|S) &= \prod_i P(w_i|S)\\
      P(D|\lnot S) &= \prod_i P(w_i|\lnot S).
    \end{split}
    \]
  \item Или:
    \[
\begin{split}
  P(S|D)&=\frac{P(S)}{P(D)}\prod_i P(w_i | S) \\
  P(\lnot S|D)&=\frac{P(\lnot S)}{P(D)}\prod_i P(w_i | \lnot S)
\end{split}
    \]
  \end{itemize}
}

\frame{
  \begin{itemize}
  \item Взяв отношение двух величин:
\[
\frac{P(S|D)}{P(\lnot S | D)} = \frac{P(S)}{P(\lnot
  S)}\prod_i\frac{P(w_i|S)}{P(w_i|\lnot S)}
\]
\item Для удобства расчётов можно перейти к логарифмам:
\[
\ln\frac{P(S|D)}{P(\lnot S|D)} = \ln\frac{P(S)}{P(\lnot S)} + \sum_i
\ln \frac{P(w_i|S)}{P(w_i|\lnot S)}
\]
\item Документ относится к категории $S$ если $P(S|D)>P(\lnot S|D)$,
  т.е.
\[
\ln\frac{P(S|D)}{P(\lnot S|D)} > 0.
\]
  \end{itemize}
}

\section{Теория информации}


\frame{
  \frametitle{Интуитивное понятние информации}
  \begin{itemize}
    \item Сколько информации содержится в строке текста? На сколько ее можно сжать без потери смысла сообщения?
    \item Есть 10 букв и алфавит из 33 символов, сколько различных строк можно представить?
    \item Допустим, мы можем передать $\Sigma^N$ различных сообщений. Только $n$ из них будут иметь смысл.
        для того, чтобы указать, какое из $n$ сообщений мы передаем, достаточно $I = \log_2n$ бит.
    \item С удвоением строки, количество возможных сообщений растет квадратично.
    \item Самое лучшее соотношение количества информации к длине сообщения у случайных строк. Для передачи 
    $2^N$ случайных равновероятных сообщения требуется ровно $N$ бит, меньше --- невозможно, больше --- избыточно.
  \end{itemize}
}

\frame[plain]{
  \frametitle{Энтропия}
  \begin{itemize}
    \item Пусть у нас есть язык, алфавит которого состоит из $k$ символов: $a_1, a_2, ... a_k$, и для каждого символа известна вероятность его появления в тексте $p_1, p_2, ..., p_k$. 
    \item Мы хотим определить количество информации в сообщении из данного языка.
    \item Если сообщение имеет длину $N$, то симолов $a_1$ в нем будет примерно $Np_1$, $a_2$ --- $Np_2$ и т.\,п.
          Посчитаем, сколько различных сообщений такого вида (с таким распределением вероятностей) может быть:
   \[
     \frac{N!}{(Np_1)!(Np_2)!...(Np_k)!}
   \]
    \item Чтобы узнать, сколько бит нужно, для передачи одного из таких сообщений, нужно взять логарифм по основанию 2. 
          Это же число можно назвать средним количеством информации в сообщении или энтропией:
   \[
      I(s) = H(s) = N \sum_{i=1}^{k} -p_i*log_2(p_i) 
   \]
  \end{itemize}
}

\frame[plain] {
  \frametitle{Энтропия}
  \begin{itemize}
    \item Чтобы узнать, сколько бит нужно, для передачи одного из таких сообщений, нужно взять логарифм по основанию 2. 
          Это же число можно назвать средним количеством информации в сообщении или энтропией:
   \[
      I(s) = H(s) = N \sum_{i=1}^{k} -p_i*log_2(p_i) 
   \]
   \item Часто энтропией называют среднее количество информации на символ. Информационная двоичная энтропия для независимых случайных событий $x$ с $k$ возможными состояниями рассчитывается по формуле:
   \[
     H(x) = - \sum_{i=1}^{k} p_i*log_2p_i
   \]
   \item Условная энтропия $n$-го порядка~--- энтропия для алфавита, в котором известны вероятности появления буквы после $n$ других.
    Условная энтропия всегда меньше безусловно, например, для русского языка $H_0 = 5, H_1 = 4.358, H_2 = 3.52, H_3 = 3.01$.
  \end{itemize}
}

\section{Сжатие текстов}

\begin{frame}
  \frametitle{Введение в проблему}

  \begin{itemize}
    \item Компрессия текстов использовалась до компьютеров: азбука
      Морзе, коды Брайля.
    \item Нужна для уменьшения объёмов хранения и передачи данных.
    \item Часть более общей задачи компрессии данных: отличие в
      возможности точного восстановления информации (для некоторых данных это не
      всегда возможно и не всегда требуется). 
  \end{itemize}
\end{frame}

\frame{
  \frametitle{Развитие}
  \begin{itemize}
  \item[1950-е] коды Хаффмана. 
  \item[1970-е] Зив-Лемпель (словарные методы) и арифметическое
    кодирование (символьные методы): адаптивное сжатие. 
  \item[1980-е] PPM (prediction by partial matching, предсказание по
    частичному совпадению).
  \item[1990-е] преобразование Барроуза-Уиллера, сжатие блоков
    данных; улучшение вычислительных характеристик алгоритмов. 
  \item На текущий момент: два бита на символ для среднего английского
    текста. 
  \item Предположительно один бит на символ недостижим. 
  \end{itemize}
}

\frame{
  \frametitle{Классификация методов компрессии текстов}
   \begin{itemize}
   \item Символьные (статистические): кодирование одного символа как можно меньшей
     последовательностью бит.
     \begin{itemize}
       \item Коды Хаффмана. 
       \item Арифметическое кодирование.
     \end{itemize}
     Основное отличие между методами в том, как вычисляются вероятности появления
     следующих символов --- построение модели текста. 
   \item Словарные: несколько часто встречающихся символов кодируются
     меньшей последовательностью бит, указывающих на соответствующий
     вход в словарь. В основном это варианты алгоритма Зива-Лемпеля.
   \end{itemize}
}



\subsection{Модели данных}
    

\frame{
  \frametitle{Модель данных}
  \begin{itemize}
    \item Задача модели --- предсказывать следующие символы в потоке
      данных. 
    \item Кодер и декодер используют одинаковую модель для упаковки
      и распаковки текста. 
    \end{itemize}
\begin{pspicture}(11,5)
%\psgrid[subgriddiv=2,griddots=10,subgriddots=10]

\fnode[framesize=1.5cm .7cm](3,4){Model1}\rput(3,4){Модель}
\fnode[framesize=1.5cm .7cm](8,4){Model2}\rput(8,4){Модель}
\fnode[framesize=1.5cm .7cm](3,2){Encoder}\rput(3,2){Кодер}
\fnode[framesize=1.5cm .7cm](8,2){Decoder}\rput(8,2){Декодер}

\pnode(0,2){TIn}
\pnode(11,2){TOut}

%\psset{linewidth=1.5pt,arrows=->}
\psset{arrows=->}

\ncline{TIn}{Encoder}\naput{Текст}
\ncline{Decoder}{TOut}\naput{Текст}
\ncline{Encoder}{Decoder}\naput{Сжатый текст}
\ncline{Model1}{Encoder}
\ncline{Model2}{Decoder}

\end{pspicture}

}

\frame{
\frametitle{Информация}
\begin{itemize}
    \item Количество бит для символа $s$ --- информация, в нём
      содержащаяся. $I(s)=-\log P(s)$.
    \item Средняя информация по всему алфавиту: 
\[
H = \sum_sP(s)I(s) = \sum_s -P(s)\log P(s)
\]
\item Энтропия --- нижний предел для алгоритмов компрессии данных,
  измеряемый в битах на символ. 
\item Коды Хаффмана часто оказываются близкими к этому пределу, однако
  в некоторых ситуациях очень неэффективны (в случаях $P(s)\rightarrow
  1$ и $I(s)\rightarrow 0$).
\item Арифметический кодер свободен от этих недостатков, но более
  медленный. 
  \end{itemize}
}

\frame{
  \frametitle{Использование моделей}
  \begin{itemize}
  \item Кодер и декодер должны использовать одну и ту же модель. 
  \item Кодер не может <<подсматривать>> вперёд для оценки
    вероятностей (потому что этого не может сделать декодер). 
  \item Символы с $P(s)=0$ не могут быть закодированы ($I(s)=\infty$).
  \item Символы с $P(s)=1$ не нуждаются в сжатии ($I(s)=0$). 
  \item Лучшая модель та, которая предсказывает настоящие символы из
    потока с максимальной вероятностью. 
  \end{itemize}
}

\frame{
  \frametitle{Контекстно ограниченная модель}
  \begin{itemize}
  \item Вероятность появления английской буквы $u$ в английском
    тексте 2\%, т.е. $I(u) \simeq 5{,}6$ битов. 
  \item Однако, после появления буквы $q$ вероятность того, что
    следующий символ будет $u$ 95\%, т.е. $P(u|q)\simeq .95$ и
     $I(u|q)\simeq 0{,}074$ бит. ($quality$, $quite$, $quiet$, \ldots).
  \item В этих случаях кодирование исключений будет требовать
    больше бит (слова $Iraq$, $Qantas$), но в среднем объём сжатых
    текстов уменьшится.
  \item Использование предыдущих $m$ символов для предсказания ---
    контекстно ограниченная модель порядка $m$. 
  \end{itemize}
}

\frame[plain]{
  \frametitle{Модель конечного автомата}

\begin{pspicture}(11,4)
%\psgrid[subgriddiv=2,griddots=10,subgriddots=10]
\cnode(3,2){.5}{S1}\rput(3,2){1}
\cnode(8,2){.5}{S2}\rput(8,2){2}
\psset{arrows=->}
\nccircle[angleA=90]{->}{S1}{.7cm}\nbput{$P(b)=\dfrac{1}{2}$}
\nccircle[angleA=270]{->}{S2}{.7cm}\nbput{$P(b)=\dfrac{1}{100}$}
\ncarc[arcangle=45]{S1}{S2}\taput{$P(a)=\dfrac{1}{2}$}
\ncarc[arcangle=45]{S2}{S1}\tbput{$P(a)=\dfrac{99}{100}$}
\end{pspicture}
\begin{itemize}
\item В состояниях содержится информация о распределении символов. 
\item Такая модель непредставима контекстно ограниченной моделью
  (отслеживает чётное или нечётное количество символов). 
\end{itemize}

%\psset{arrows=->,nodesep=0.05}

%\nccircle[angleA=90,nodesep=3pt]{->}{1,1}{.7cm}
%\ncarc[angleA=70,angleB=110]{1,1}{1,1}
%\ncdiag{1,1}{1,3}
%\ncdiag{1,3}{1,1}
%\ncdiag{1,3}{1,3}

%\ncangles[armA=0.8,armB=0,angleA=70,angleB=110,linearc=0.22]{1,1}{1,1}
%\ncdiag[arm=0.9,angleA=70,angleB=110,linearc=0.3]{1,2}{1,2}
%\ncdiag[arm=0.8,angleA=50,angleB=130,linearc=0.25]{1,3}{1,3}
%\ncdiag[arm=0.8,angleA=50,angleB=130,linearc=0.28]{1,4}{1,4}
}

\frame{
  \frametitle{Вероятностное распределение символов}
  \begin{itemize}
  \item Кодер должен закодировать обрабатываемый символ и только после
    этого изменить распределение. 
  \item Декодер раскодирует символ в соответствии с текущим
    распределением, после чего делает аналогичное изменение. 
  \item Предполагается безошибочная передача данных между кодером и
    декодером. 
  \item Если известна формальная грамматика текста (C, Java), то её
    можно использовать для моделирования. 
  \end{itemize}
}

\subsection{Адаптивные модели}

\frame{
  \frametitle{Оценка вероятностей}
  \begin{itemize}
  \item Статическое моделирование: во время разработки модели
    назначается распределение и используется для всех
    входных текстов. 
  \item Полустатическое моделирование: создание распределения
    вероятностей на основе входного файла. Требуется два прохода по
    файлу 
    (сбор статистики и сжатие) а так же явная передача данных модели
    от кодера к декодеру. 
  \item Адаптивное моделирование: измененение распределения
    вероятностей во время сжатия входных данных. 
  \end{itemize}
}

\frame{
  \frametitle{Пример}

  \begin{quote}
  ``I never heerd a silful old married feller of twenty years'
  standing pipe ``my wife'' in a more used note than 'a did,'', said
  Jacob Smallbury. It migh
\end{quote}
Для модели нулевого порядка:
\begin{itemize}
  \item $P(t) = 49{,}983/768{,}078 = 6{.}5\%$. 
  \item $P(e) = 9{.}4\%$. 
  \item $P(x) = 0{.}11\%$. 
\end{itemize}
Так как следующая буква t, декодер закодирует её $-\log 0.065=3.94$ битами. 
}

\frame[plain]{
  \frametitle{Символы с нулевой частотой}
  Необходимо избегать ситуаций, при которых некоторые символы
  оцениваются с нулевой вероятностью. 
  \begin{itemize}
  \item В примере ни разу не встретилась Z, так что простая оценка
    $P(Z)=0$ и эта буква не сможет быть закодирована. Строка MighZ
    довольно необычна, но не невероятна. 
\pause
  \item Один из способов решения проблемы --- назначение всем
    символам, которые до сих пор не встречались, суммарной единичной
    частоты:
\begin{enumerate}
  \item В примере 82 символа встретились, 46 ASCII-символов не
    встречались не разу. Тем самым, они суммарно должны разделить
    между собой $1/768{,}079$. 
  \item  Т.е., $P(Z)=1/(46\times 768{,}079) = 1/35{,}331{,}634$, что
    соответствует $25.07$ битам. 
\end{enumerate}
\pause
  \item Другой споосб --- назначение всем символам изначальной
    единичной частоты. Тогда $P(Z)=1/768{,}206$, соответствует $19.6$
    битам.
\pause
  \item Выбор способа борьбы с проблемой существенно влияет на
    эффективность сжатия коротких текстов и сглаживается для больших. 
  \end{itemize}
}

\frame{
  \frametitle{Адаптивные модели высоких порядков}
  \begin{quote}
  ``I never heerd a silful old married feller of twenty years'
  standing pipe ``my wife'' in a more used note than 'a did,'', said
  Jacob Smallbury. It migh
\end{quote}
\begin{itemize}
  \item Модель первого порядка:
    \begin{enumerate}
      \item $c[h] = 37{,}525$, $c[t|h] = 1{,}133$. 
      \item Опуская проблему
    символов с нулевой частотой, $P(t|h) = 1{,}133/37{,}525=3{.}02\%
    $, 
    \item Будет закодировано $5.05$ битами (хуже, чем модель нулевого
      порядка для этого случая, т.к. за $h$ чаще следует~$e$). 
    \end{enumerate}
  \item Модель второго порядка:
    \begin{enumerate}
    \item $c[gh]=1{,}754$, $c[t|gh]=1{,}129$.
    \item $P(t|gh)=64{.}4\%$.
    \item Будет использован код длиною $0.636$ бит. 
    \end{enumerate}
\end{itemize}
}

\frame{
  \frametitle{Изменение структуры модели}
  \begin{itemize}
  \item В адаптивной модели может меняться не только распределение
    частот, но и структура модели. 
  \item Например, в модели конечного автомата могут добавляться новые
    состояния и генерироваться новые переходы в эти состояния. 
  \item Обычно структура менятся в тех местах модели, которые часто
    используются. 
  \end{itemize}
}

\frame{
  \frametitle{Выводы}
  \begin{itemize}
    \item Адаптивная модель позволяет добиться хорошего качества
      сжатия. 
    \item Однако, с её использованием нельзя получить доступ к
      случайному месту внутри сжатого файла: нужно пройти весь путь от
      начала сжатого файла до этого места, чтобы получить нужную
      модель. 
  \end{itemize}
}



\end{document}
    
